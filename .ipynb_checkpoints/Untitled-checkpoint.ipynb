{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acknowledged-constraint",
   "metadata": {},
   "source": [
    "# Hemuppgift tf*idf\n",
    "\n",
    "Grupp: Delat med noll\n",
    "\n",
    "Medlemmar: Andreas Gustafsson, Oscar Widing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-acceptance",
   "metadata": {},
   "source": [
    "We need the following libraries to run the code:\n",
    "\n",
    "- pip install bs4\n",
    "- pip install nltk\n",
    "- pip install numpy\n",
    "- pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-switzerland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-eclipse",
   "metadata": {},
   "source": [
    "# Example 1:\n",
    "\n",
    "Compare content of two books from gutenberg.org\n",
    "\n",
    "Start with downloading content from the given links. We also return the total amount of books in our corpus. Knowing we are only comparing 2 books it's not really necessary for this comparison but when comparing larger document collections it's good to get the corpus size since we will use this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books(links):\n",
    "    books = []\n",
    "    \n",
    "    for link in links:\n",
    "        page = requests.get(link)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # content of book\n",
    "        story = [p.get_text() for p in soup.select(\"p\")]\n",
    "                \n",
    "        books.append(str(story))\n",
    "    \n",
    "    return books, len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_to_download = [\n",
    "    'https://www.gutenberg.org/files/30667/30667-h/30667-h.htm',\n",
    "    'https://www.gutenberg.org/files/7766/7766-h/7766-h.htm'\n",
    "]\n",
    "\n",
    "raw_documents, n = get_books(books_to_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-spelling",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "After the download of the data. Preprocessing is necessary to get the most accurate result when comparing the documents.\n",
    "We want the 'essence' of the content. We want all words that are similar to be counted as the same word, no matter the plural ending, capital letters, written numbers etc.\n",
    "\n",
    "- We start with removing all linebreaks such as \"\\n\" and \"\\r\"\n",
    "- Convert all letters to lowercase\n",
    "    - So that Car and car wont count as two different words.\n",
    "- Remove all symbols such as !\\#%&()*+-./:;<=>?@^_{|}~\n",
    "- Remove stopwords\n",
    "    - Removing stopwords such as \"The\" and \"and\" will give a better result when comparing two documents. Such stopwords most         likely exists in all documents ranging from science-fiction to history books and doesnÂ´t give any indication of the what       document is about. Therefore there is no point in comparing them. \n",
    "- Remove all apostrophe\n",
    "    - The lib nltk doesn't count dont as a stopword. Thats why we remove stopwords before apostrophe.\n",
    "- Convert int to str numbers\n",
    "    - So that 100 = one hundred, same reason as converting to lowercase\n",
    "- Lemmetize words\n",
    "    - change all words to its most 'basic' form. copies -> copy\n",
    "- Stemming words\n",
    "    - Also reduces the word to its most 'basic' form but doesn't necessary make it to an actual word. copies -> copi\n",
    "    - We lemmetize before stemming because words such as caring -> car\n",
    "- Remove symbols again\n",
    "    - num2word lib changes 100500 -> one hundred thousand, five hundred.\n",
    "- Remove single character words\n",
    "    - cleaning up leftovers from preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def remove_single_char(data):\n",
    "    return [word for word in str(data).split() if len(word) > 1]\n",
    "\n",
    "\n",
    "def lemmetizer(data):\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmer.lemmatize(w)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def remove_linebreaks(data):\n",
    "    clean_data = list(map((lambda x: x.replace('\\\\n', ' ').replace('\\\\r', ' ')), data.split()))\n",
    "    return \" \".join(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    print(f\"book was {len(data.split())} words long\")\n",
    "    \n",
    "    data = remove_linebreaks(data)\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_symbols(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = lemmetizer(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_symbols(data)\n",
    "    data = remove_single_char(data)\n",
    "\n",
    "    print(f\"book is now {len(data)} words long\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [preprocess(document) for document in raw_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-truth",
   "metadata": {},
   "source": [
    "# Calculate TF\n",
    "\n",
    "We calculate term frequency for every word in every document. This is the value of how many times a word is present in a document compared to how many words there are in the document.\n",
    "\n",
    "This gives an indication of how relevant the word is to the document. The more times the word occurs the higher weight the word is given. Meaning more relevant to the document.\n",
    "\n",
    "We create a dictionary that holds all the words in all of the document as keys. The value is the index of every book in the collection and the DF.\n",
    "\n",
    "Every book is also a dictionary that holds a list as value, the list will initially only be given the TF for that word and book. When TF is assigned to a word and bookindex we also increase the DF value by one. To keep track in how many books the word is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(documents, n, show=True):\n",
    "    word_dict = defaultdict(dict)\n",
    "    for book_index, document in enumerate(documents):\n",
    "        for word in document:\n",
    "            if word not in word_dict:\n",
    "                book_indexes = {i: [0] for i in range(n)}\n",
    "                word_dict[word] = book_indexes\n",
    "                word_dict[word]['DF'] = 0\n",
    "            if word_dict[word][book_index][0] == 0:\n",
    "                word_dict[word][book_index] = [document.count(word) / len(document)]\n",
    "                word_dict[word]['DF'] += 1\n",
    "    if show:\n",
    "        [print(word, word_dict[word]) for word in list(word_dict.keys())[3:8]]\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_document = calculate_tf(documents, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-activation",
   "metadata": {},
   "source": [
    "# Calculate IDF\n",
    "\n",
    "In this step we will calculate tf x idf and add that result to every word and book index. We already know the tf for every word and book. What we do now is calculate inverse document frequency and multiply that with tf and add it to the list value. \n",
    "\n",
    "Inverse document frequency will give the most unique words the highest weight, since we want a high tfxidf to represent a unique and common word to that document. \n",
    "\n",
    "We divide the n, the total number of documents with the document frequency that we calculated earlier. This means that if a word is unique to only one document a higher weight is given to that word. Meaning that word is more important to that document since its not present in any other documents.\n",
    "\n",
    "We've choosen to use a normalization to our calculation. We're adding +1 to n, df and the result of log(). When we are comparing only two documents this is necessary when calculating cosine similarity since we take the dot product of the vectors.\n",
    "\n",
    "Otherwise the result would always be 0. If we calculate tfxidf based on log(n/df). But this is only an issue when comparing only two documents. As mentioned, to counteract that we smoothen the calculation and add 1.\n",
    "\n",
    "Example (based on count vectorization, not taken tfxidf into consideration for the example):\n",
    "sentence_1 = \"My name is Andreas\"\n",
    "sentence_2 = \"My name is Oscar\"\n",
    "\n",
    "vector_1 = [0, 0, 0, 1, 0] vector_2 = [0, 0, 0, 0, 1]\n",
    "\n",
    "dot product will be 0 since there is no \"weight\" at all to words that appear in all documents. Even though the sentences are very similar the cosine similarity would be 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf(tf_document, n, show=True):\n",
    "    for word, book_indexes in tf_document.items():\n",
    "        for book_index, tf in book_indexes.items():\n",
    "            if book_index != 'DF':\n",
    "                tf_document[word][book_index].append(tf[0] * (math.log(n + 1 / tf_document[word]['DF'] + 1) + 1))\n",
    "    if show:\n",
    "        [print(word, tf_document[word]) for word in list(tf_document.keys())[3:8]]\n",
    "    return tf_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_document = calculate_tf_idf(tf_document, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-throw",
   "metadata": {},
   "source": [
    "# Vectorize text\n",
    "\n",
    "We create a vector for each document. The 'base' of that vector is the tfxidf value for every unique word across all the documents.\n",
    "\n",
    " - Example:\n",
    "     sentence_1 = \"My name is Andreas\"\n",
    "     sentence_2 = \"His name is Oscar\"\n",
    "     \n",
    "     unique words = [my, name, is, andreas, his, oscar]\n",
    "     \n",
    "     tf_idf sentence_1 = [0.29, 0.25, 0.25, 0.29, 0, 0]\n",
    "     tf_idf sentence_2 = [0, 0.25, 0.25, 0, 0.29, 0.29]\n",
    "     \n",
    "We first create an empty vector for all the documents. Then we loop through our dictionary that has all unique words and each tfxidf for every document. Then append the value to our vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tf_idf(tf_idf_document, n):\n",
    "    vector_doc = [[] for _ in range(n)]\n",
    "    [vector_doc[i].append(tf_idf_document[word][i][1]) for i in range(n) for word in tf_idf_document]\n",
    "    return vector_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_document = vectorize_tf_idf(tf_idf_document, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-officer",
   "metadata": {},
   "source": [
    "# Calculate similarity with cosine\n",
    "\n",
    "Now that we have a vector for every document we can compare the angle between those vectors to see how similar they are. \n",
    "first we need to calculate the angle, or to be precise the value for cosine of the angle, as a value ranging from 0 (not related) to 1 (the same)\n",
    "\n",
    "We will use the following formula:\n",
    "     u *(dot) v = ||u|| ||v|| cos(angle)\n",
    "\n",
    "Since ||u|| = (u *(dot) u)^0.5 we can write a function that calculates the dot product and use that for all calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v1, v2):\n",
    "    return sum(v * u for v, u in zip(v1, v2))\n",
    "\n",
    "\n",
    "def cosine_calculation(u, v):\n",
    "    return dot(u, v) / ((dot(u, u)**0.5) * (dot(v, v)**0.5))\n",
    "\n",
    "\n",
    "def cosine_comparison(comparison_doc, v_doc):\n",
    "    return [cosine_calculation(comparison_doc, doc) for doc in v_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_comparison(vectorized_document[0], vectorized_document)\n",
    "\n",
    "print(cos_sim[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-relevance",
   "metadata": {},
   "source": [
    "# Example two, recommendation\n",
    "\n",
    "We can use cosine simularity to compare the content of books and see which books resembles the compared book the most.\n",
    "we use the same principle as before, only this time we create a larger collection of book (we have prepared 10 titles). And compare all books with the book of your choice. We run the algorithm and sort the answer from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books(book_to_compare):\n",
    "    regex = \"(?<=of )(.*)(?=, by)\"\n",
    "    books = []\n",
    "    titles = []\n",
    "    links = [\n",
    "        'https://www.gutenberg.org/files/30667/30667-h/30667-h.htm',\n",
    "        \"https://www.gutenberg.org/files/13670/13670-h/13670-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/7423/7423-h/7423-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/6768/6768-h/6768-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/4682/4682-h/4682-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/3829/3829-h/3829-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/16921/16921-h/16921-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/25550/25550-h/25550-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/31619/31619-h/31619-h.htm\",\n",
    "        \"https://www.gutenberg.org/files/84/84-h/84-h.htm\",\n",
    "    ]\n",
    "    \n",
    "    for link in links:\n",
    "        page = requests.get(link)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # book title\n",
    "        title = soup.find('title')\n",
    "        fixed_title = re.search(regex, title.string)[0]\n",
    "        # book content\n",
    "        story = [p.get_text() for p in soup.select(\"p\")]\n",
    "        \n",
    "        books.append(str(story))\n",
    "        titles.append(str(fixed_title))\n",
    "    \n",
    "    # Get compared book content\n",
    "    page = requests.get(book_to_compare)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    title = 'Book_to_compare'\n",
    "    story = [p.get_text() for p in soup.select(\"p\")]\n",
    "    \n",
    "    books.append(str(story)), titles.append(title)\n",
    "    \n",
    "    return books, len(books), titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-viking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_to_compare = 'https://www.gutenberg.org/files/7766/7766-h/7766-h.htm'\n",
    "\n",
    "raw_documents, n, titles = get_books(book_to_compare)\n",
    "documents = [preprocess(document) for document in raw_documents]\n",
    "tf_document = calculate_tf(documents, n, show=False)\n",
    "tf_idf_document = calculate_tf_idf(tf_document, n, show=False)\n",
    "vectorized_document = vectorize_tf_idf(tf_idf_document, n)\n",
    "cos_sim = cosine_comparison(vectorized_document[-1], vectorized_document)\n",
    "\n",
    "print('From our major database of books, in descending order, the most similar are:')\n",
    "results = sorted(zip(titles, cos_sim), key=lambda x: x[1], reverse=True)[1:]\n",
    "for result in results:\n",
    "    print(f'{result[0]}: Score {round(result[1], 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-lunch",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "What is tf_idf, what is the math and how does in work?\n",
    "    - A mathematical way to describe similarities between texts. Where every word is given a weight, the higher the weight the       more unique or frequent the word is to that particular document. Making it more relevant to the document compared to the       collection. This is a great way to compare content of words in a text which is simple and effective.\n",
    "\n",
    "Advantages with tf_idf?\n",
    "    - Simple and efficient. Easy to set up and use and gets a fairly accurate result without the need for massive model               trainings.\n",
    "\n",
    "Disadvantages?\n",
    "    - Only differates words but not the context of the text. For example, two text that have the same meaning but are written         in different ways would get a low score, tf_idf interprets this as two very different texts.\n",
    "      Its hard to interpret the value of the score in how similar they are but presents a good picture of which book is the           most similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
